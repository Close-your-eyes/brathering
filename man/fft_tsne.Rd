% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fft_tsne.R
\name{fft_tsne}
\alias{fft_tsne}
\title{FFT-accelerated Interpolation-based t-SNE (FIt-SNE)}
\usage{
fft_tsne(
  X,
  dims = 2,
  perplexity = 30,
  theta = 0.5,
  max_iter = 750,
  fft_not_bh = TRUE,
  ann_not_vptree = TRUE,
  stop_early_exag_iter = 250,
  exaggeration_factor = 12,
  no_momentum_during_exag = 0,
  start_late_exag_iter = -1,
  late_exag_coeff = 1,
  mom_switch_iter = 250,
  momentum = 0.5,
  final_momentum = 0.8,
  learning_rate = "auto",
  n_trees = 50,
  search_k = -1,
  rand_seed = -1,
  nterms = 3,
  intervals_per_integer = 1,
  min_num_intervals = 50,
  K = -1,
  sigma = -30,
  initialization = "pca",
  max_step_norm = 5,
  load_affinities = NULL,
  fast_tsne_path = system2("which", "fast_tsne", stdout = TRUE),
  nthreads = 0,
  perplexity_list = NULL,
  get_costs = FALSE,
  df = 1
)
}
\arguments{
\item{X}{numeric matrix with observations as rows}

\item{dims}{dimensionality of the embedding}

\item{perplexity}{perplexity is used to determine the bandwidth of the
Gaussian kernel in the input space}

\item{theta}{Set to 0 for exact. If non-zero, then will use either
Barnes Hut or FIt-SNE based on fft_not_bh. If Barnes Hut, then
this determines the accuracy of BH approximation.}

\item{max_iter}{Number of iterations of t-SNE to run}

\item{fft_not_bh}{if theta is nonzero, this determines whether to
use FIt-SNE or Barnes Hut approximation. Default is FIt-SNE.
set to be True for FIt-SNE}

\item{ann_not_vptree}{use vp-trees (as in bhtsne) or approximate nearest
neighbors (default). set to be True for approximate nearest neighbors}

\item{stop_early_exag_iter}{When to switch off early exaggeration.}

\item{exaggeration_factor}{coefficient for early exaggeration (>1)}

\item{no_momentum_during_exag}{Set to 0 to use momentum
and other optimization tricks. 1 to do plain, vanilla
gradient descent (useful for testing large exaggeration
coefficients)}

\item{start_late_exag_iter}{When to start late exaggeration. 'auto' means
that late exaggeration is not used, unless late_exag_coeff>0. In that
case, start_late_exag_iter is set to stop_early_exag_iter. Otherwise,
set to equal the iteration at which late exaggeration should begin.}

\item{late_exag_coeff}{Late exaggeration coefficient.
Set to -1 to not use late exaggeration.}

\item{mom_switch_iter}{Iteration after which the final momentum is used}

\item{momentum}{Momentum used in the first part of the optimization}

\item{final_momentum}{Momentum used in the final part of the optimization}

\item{learning_rate}{Set to desired learning rate or 'auto', which
sets learning rate to N/exaggeration_factor where N is the sample size, or to 200 if
N/exaggeration_factor < 200.}

\item{n_trees}{used in conjunction with search_k}

\item{search_k}{if (search_k == -1) {
  if (perplexity > 0) {
    search_k <- n_trees * perplexity * 3
  } else if (perplexity == 0) {
    search_k <- n_trees * max(perplexity_list) * 3
  } else {
    search_k <- n_trees * K
  }
}}

\item{rand_seed}{random seed. passed to set.seed when not equal to -1.}

\item{nterms}{If using FIt-SNE, this is the number of
interpolation points per sub-interval}

\item{intervals_per_integer}{See min_num_intervals}

\item{min_num_intervals}{Let maxloc = ceil(max(max(X)))
and minloc = floor(min(min(X))). i.e. the points are in
a [minloc]^no_dims by [maxloc]^no_dims interval/square.
The number of intervals in each dimension is either
min_num_intervals or ceil((maxloc -
minloc)/intervals_per_integer), whichever is
larger. min_num_intervals must be an integer >0,
and intervals_per_integer must be >0. Default:
min_num_intervals=50, intervals_per_integer =
1}

\item{K}{Number of nearest neighbors to get when using fixed sigma}

\item{sigma}{Fixed sigma value to use when perplexity==-1}

\item{initialization}{'pca', 'random', or N x no_dims array to initialize the solution.}

\item{max_step_norm}{Maximum distance that a point is allowed to move on
one iteration. Larger steps are clipped to this value. This prevents
possible instabilities during gradient descent.  Set to -1 to switch it
off.}

\item{load_affinities}{If 1, input similarities are loaded from a file and not computed
If 2, input similarities are saved into a file.
If 0, affinities are neither saved nor loaded}

\item{fast_tsne_path}{path to fast_tsne binary}

\item{nthreads}{Number of threads to use. Setting to 0 corresponds to detecting and using all available cores.
May require OpenMP.}

\item{perplexity_list}{if perplexity==0 then perplexity combination will
be used with values taken from perplexity_list.}

\item{df}{Degree of freedom of t-distribution, must be greater than 0.
Values smaller than 1 correspond to heavier tails, which can often
resolve substructure in the embedding. See Kobak et al. (2019) for
details.}
}
\value{
matrix with tsne dimensions
}
\description{
Faster alternative to Rtsne::Rtsne.
Modified function definition and import as R function with few formal modifications.. Original from here:
https://github.com/KlugerLab/FIt-SNE. Paper: https://www.nature.com/articles/s41592-018-0308-4.
Prerequisites:
On Mac, install fftw (e.g https://ports.macports.org/port/fftw-3/).
Then git clone https://github.com/KlugerLab/FIt-SNE.git. Then navigate to FIt-SNE folder
and run g++ -std=c++11 -O3  src/sptree.cpp src/tsne.cpp src/nbodyfft.cpp  -o bin/fast_tsne -pthread -lfftw3 -lm -Wno-address-of-packed-member.
Then copy binary to path:
e.g. sudo cp /Users/chris/FIt-SNE/bin/fast_tsne /usr/local/bin/
On Windows, buy a Mac.
}
